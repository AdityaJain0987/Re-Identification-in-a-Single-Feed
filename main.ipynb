{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e88c2eca",
   "metadata": {},
   "source": [
    "# YOLO Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b684e243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n.pt to 'yolov8n.pt'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6.25M/6.25M [00:05<00:00, 1.29MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing video with YOLO...\n",
      "Processed 0 frames\n",
      "Processed 30 frames\n",
      "Processed 60 frames\n",
      "Processed 90 frames\n",
      "Processed 120 frames\n",
      "Processed 150 frames\n",
      "Processed 180 frames\n",
      "Processed 210 frames\n",
      "Processed 240 frames\n",
      "Processed 270 frames\n",
      "Processed 300 frames\n",
      "Processed 330 frames\n",
      "Processed 360 frames\n",
      "YOLO detection complete! Saved to yolo_detections.json\n",
      "Total frames processed: 375\n",
      "Now you can run the ReID tracking system!\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import json\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "\n",
    "def run_yolo_detection(video_path, output_json_path, model_path='yolov8n.pt'):\n",
    "    \"\"\"\n",
    "    Run YOLO detection on video and save results in JSON format\n",
    "    \n",
    "    Args:\n",
    "        video_path: Path to input video\n",
    "        output_json_path: Path to save JSON detections\n",
    "        model_path: Path to YOLO model (or model name)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load YOLO model\n",
    "    model = YOLO(model_path)\n",
    "    \n",
    "    # Open video\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    all_detections = {}\n",
    "    frame_idx = 0\n",
    "    \n",
    "    print(\"Processing video with YOLO...\")\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Run YOLO detection\n",
    "        results = model(frame, verbose=False)\n",
    "        \n",
    "        # Extract person detections (class 0 is person in COCO)\n",
    "        frame_detections = []\n",
    "        \n",
    "        for result in results:\n",
    "            boxes = result.boxes\n",
    "            if boxes is not None:\n",
    "                for box in boxes:\n",
    "                    # Filter for person class (class 0) and confidence > 0.5\n",
    "                    if box.cls.item() == 0 and box.conf.item() > 0.5:\n",
    "                        # Get bounding box coordinates\n",
    "                        x1, y1, x2, y2 = box.xyxy[0].tolist()\n",
    "                        frame_detections.append([x1, y1, x2, y2])\n",
    "        \n",
    "        # Store detections for this frame\n",
    "        all_detections[str(frame_idx)] = frame_detections\n",
    "        \n",
    "        if frame_idx % 30 == 0:\n",
    "            print(f\"Processed {frame_idx} frames\")\n",
    "        \n",
    "        frame_idx += 1\n",
    "    \n",
    "    cap.release()\n",
    "    \n",
    "    # Save detections to JSON\n",
    "    with open(output_json_path, 'w') as f:\n",
    "        json.dump(all_detections, f, indent=2)\n",
    "    \n",
    "    print(f\"YOLO detection complete! Saved to {output_json_path}\")\n",
    "    print(f\"Total frames processed: {frame_idx}\")\n",
    "    \n",
    "    return all_detections\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    video_path = \"15sec_input_720p.mp4\"\n",
    "    detections_output = \"yolo_detections.json\"\n",
    "    \n",
    "    # Run YOLO detection\n",
    "    detections = run_yolo_detection(video_path, detections_output)\n",
    "    \n",
    "    print(\"Now you can run the ReID tracking system!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2b9acb",
   "metadata": {},
   "source": [
    "# Player Re-ID Class using ViT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46b7f5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import numpy as np\n",
    "from transformers import ViTModel, ViTFeatureExtractor\n",
    "from collections import defaultdict, deque\n",
    "import json\n",
    "import os\n",
    "\n",
    "class PlayerReID:\n",
    "    \"\"\"\n",
    "    Simple but effective player re-ID using pre-trained ViT.\n",
    "    Optimized for GPU execution.\n",
    "    \"\"\"\n",
    "    def __init__(self, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "        self.device = device\n",
    "\n",
    "        # Load pre-trained ViT model and move to device\n",
    "        self.feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224')\n",
    "        self.model = ViTModel.from_pretrained('google/vit-base-patch16-224')\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "        self.player_features = defaultdict(deque)\n",
    "        self.player_metadata = {}\n",
    "        self.next_id = 0\n",
    "\n",
    "        # Configuration parameters by optuna\n",
    "        self.max_history = 36\n",
    "        self.similarity_threshold = 0.414 \n",
    "        self.max_disappeared = 54\n",
    "\n",
    "        self.last_positions = {}\n",
    "        self.disappeared_count = defaultdict(int)\n",
    "\n",
    "    def extract_features(self, image_crop):\n",
    "        \"\"\"Extract features from player crop using pre-trained ViT\"\"\"\n",
    "        if image_crop.size == 0:\n",
    "            return None\n",
    "\n",
    "        image_crop = cv2.resize(image_crop, (224, 224))\n",
    "        image_crop = cv2.cvtColor(image_crop, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        inputs = self.feature_extractor(images=image_crop, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            features = outputs.last_hidden_state[:, 0, :]  # CLS token\n",
    "            features = F.normalize(features, p=2, dim=1)\n",
    "\n",
    "        return features.cpu().numpy().flatten()\n",
    "\n",
    "    def compute_similarity(self, feat1, feat2):\n",
    "        \"\"\"Compute cosine similarity using GPU-enabled PyTorch\"\"\"\n",
    "        feat1 = torch.tensor(feat1, device=self.device)\n",
    "        feat2 = torch.tensor(feat2, device=self.device)\n",
    "        similarity = F.cosine_similarity(feat1.unsqueeze(0), feat2.unsqueeze(0))\n",
    "        return similarity.item()\n",
    "\n",
    "    def get_temporal_features(self, player_id):\n",
    "        \"\"\"Aggregate historical features using weighted average on GPU\"\"\"\n",
    "        if player_id not in self.player_features:\n",
    "            return None\n",
    "\n",
    "        features_list = list(self.player_features[player_id])\n",
    "        if not features_list:\n",
    "            return None\n",
    "\n",
    "        weights = np.exp(np.linspace(-1, 0, len(features_list)))\n",
    "        weights = weights / weights.sum()\n",
    "\n",
    "        features_tensor = torch.tensor(features_list, dtype=torch.float32, device=self.device)\n",
    "        weights_tensor = torch.tensor(weights, dtype=torch.float32, device=self.device).unsqueeze(1)\n",
    "\n",
    "        temporal_features = torch.sum(features_tensor * weights_tensor, dim=0)\n",
    "        temporal_features = F.normalize(temporal_features, p=2, dim=0)\n",
    "\n",
    "        return temporal_features.cpu().numpy()\n",
    "\n",
    "    def motion_consistency_check(self, player_id, current_bbox):\n",
    "        \"\"\"Check if detection matches expected player motion\"\"\"\n",
    "        if player_id not in self.last_positions:\n",
    "            return True\n",
    "\n",
    "        last_bbox = self.last_positions[player_id]\n",
    "        last_center = ((last_bbox[0] + last_bbox[2]) / 2, (last_bbox[1] + last_bbox[3]) / 2)\n",
    "        current_center = ((current_bbox[0] + current_bbox[2]) / 2, (current_bbox[1] + current_bbox[3]) / 2)\n",
    "\n",
    "        distance = np.sqrt((current_center[0] - last_center[0]) ** 2 + (current_center[1] - last_center[1]) ** 2)\n",
    "        max_movement = 100  # pixels\n",
    "\n",
    "        return distance <= max_movement\n",
    "\n",
    "    def find_best_match(self, query_features, current_bbox):\n",
    "        \"\"\"Find the best matching player based on similarity and motion\"\"\"\n",
    "        if query_features is None:\n",
    "            return None, 0.0\n",
    "\n",
    "        best_id = None\n",
    "        best_similarity = 0.0\n",
    "\n",
    "        for player_id in self.player_features:\n",
    "            if self.disappeared_count[player_id] > self.max_disappeared:\n",
    "                continue\n",
    "\n",
    "            temporal_features = self.get_temporal_features(player_id)\n",
    "            if temporal_features is None:\n",
    "                continue\n",
    "\n",
    "            similarity = self.compute_similarity(query_features, temporal_features)\n",
    "\n",
    "            if not self.motion_consistency_check(player_id, current_bbox):\n",
    "                similarity *= 0.5\n",
    "\n",
    "            if similarity > best_similarity:\n",
    "                best_similarity = similarity\n",
    "                best_id = player_id\n",
    "\n",
    "        return best_id, best_similarity\n",
    "\n",
    "    def update_player(self, player_id, features, bbox):\n",
    "        \"\"\"Update player database with new features and position\"\"\"\n",
    "        self.player_features[player_id].append(features)\n",
    "        if len(self.player_features[player_id]) > self.max_history:\n",
    "            self.player_features[player_id].popleft()\n",
    "\n",
    "        self.last_positions[player_id] = bbox\n",
    "        self.disappeared_count[player_id] = 0\n",
    "\n",
    "        if player_id not in self.player_metadata:\n",
    "            self.player_metadata[player_id] = {'first_seen': 0, 'total_detections': 0}\n",
    "        self.player_metadata[player_id]['total_detections'] += 1\n",
    "\n",
    "    def process_frame(self, frame, detections, frame_idx):\n",
    "        \"\"\"Process single frame and assign player IDs\"\"\"\n",
    "        player_ids = []\n",
    "\n",
    "        for player_id in self.player_features:\n",
    "            self.disappeared_count[player_id] += 1\n",
    "\n",
    "        for bbox in detections:\n",
    "            x1, y1, x2, y2 = bbox\n",
    "            player_crop = frame[int(y1):int(y2), int(x1):int(x2)]\n",
    "\n",
    "            features = self.extract_features(player_crop)\n",
    "            if features is None:\n",
    "                player_ids.append(-1)\n",
    "                continue\n",
    "\n",
    "            best_id, similarity = self.find_best_match(features, bbox)\n",
    "\n",
    "            if best_id is not None and similarity > self.similarity_threshold:\n",
    "                self.update_player(best_id, features, bbox)\n",
    "                player_ids.append(best_id)\n",
    "            else:\n",
    "                new_id = self.next_id\n",
    "                self.next_id += 1\n",
    "                self.update_player(new_id, features, bbox)\n",
    "                self.player_metadata[new_id]['first_seen'] = frame_idx\n",
    "                player_ids.append(new_id)\n",
    "\n",
    "        return player_ids\n",
    "\n",
    "    def cleanup_old_players(self):\n",
    "        \"\"\"Remove stale player tracks\"\"\"\n",
    "        to_remove = [pid for pid, count in self.disappeared_count.items() if count > self.max_disappeared * 2]\n",
    "        for pid in to_remove:\n",
    "            del self.player_features[pid]\n",
    "            del self.disappeared_count[pid]\n",
    "            self.last_positions.pop(pid, None)\n",
    "\n",
    "    def save_results(self, output_path):\n",
    "        \"\"\"Save player tracking results\"\"\"\n",
    "        results = {\n",
    "            'player_metadata': self.player_metadata,\n",
    "            'total_players': self.next_id,\n",
    "            'configuration': {\n",
    "                'similarity_threshold': self.similarity_threshold,\n",
    "                'max_history': self.max_history,\n",
    "                'max_disappeared': self.max_disappeared\n",
    "            }\n",
    "        }\n",
    "        with open(output_path, 'w') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "\n",
    "\n",
    "def process_video_with_yolo_detections(video_path, detections_file, output_video_path):\n",
    "    \"\"\"\n",
    "    Process video with YOLO detections and track players using re-ID\n",
    "    \"\"\"\n",
    "    with open(detections_file, 'r') as f:\n",
    "        all_detections = json.load(f)\n",
    "\n",
    "    reid_system = PlayerReID()\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
    "\n",
    "    frame_idx = 0\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame_detections = all_detections.get(str(frame_idx), [])\n",
    "        player_ids = reid_system.process_frame(frame, frame_detections, frame_idx)\n",
    "\n",
    "        for detection, player_id in zip(frame_detections, player_ids):\n",
    "            x1, y1, x2, y2 = detection\n",
    "            color = (0, 255, 0) if player_id != -1 else (0, 0, 255)\n",
    "            cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), color, 2)\n",
    "            if player_id != -1:\n",
    "                cv2.putText(frame, f'Player {player_id}', (int(x1), int(y1) - 10),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
    "\n",
    "        out.write(frame)\n",
    "\n",
    "        if frame_idx % 30 == 0:\n",
    "            reid_system.cleanup_old_players()\n",
    "            print(f\"Processed {frame_idx} frames\")\n",
    "\n",
    "        frame_idx += 1\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    reid_system.save_results('tracking_results.json')\n",
    "    print(f\"Total unique players detected: {reid_system.next_id}\")\n",
    "    return reid_system\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4c8dd4",
   "metadata": {},
   "source": [
    "# Main Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f06abb59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Optimization Checks:\n",
      "PyTorch version: 2.5.1+cu121\n",
      "CUDA available: True\n",
      "CUDA version: 12.1\n",
      "GPU device: NVIDIA GeForce RTX 3050 6GB Laptop GPU\n",
      "GPU memory: 6.0 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 frames\n",
      "Processed 30 frames\n",
      "Processed 60 frames\n",
      "Processed 90 frames\n",
      "Processed 120 frames\n",
      "Processed 150 frames\n",
      "Processed 180 frames\n",
      "Processed 210 frames\n",
      "Processed 240 frames\n",
      "Processed 270 frames\n",
      "Processed 300 frames\n",
      "Processed 330 frames\n",
      "Processed 360 frames\n",
      "Total unique players detected: 38\n",
      "Processing complete!\n",
      "Results saved to: tracking_results.json\n",
      "Output video saved to: tracked_output_new.mp4\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "        # GPU optimization checks\n",
    "    print(\"GPU Optimization Checks:\")\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"CUDA version: {torch.version.cuda}\")\n",
    "        print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "    \n",
    "    video_path = \"15sec_input_720p.mp4\"\n",
    "    # detections_file = \"tracking_results.json\"  # You need to create this\n",
    "    detections_file = \"yolo_detections.json\"  # You need to create this\n",
    "    output_video_path = \"tracked_output_new.mp4\"\n",
    "    \n",
    "    # Process video\n",
    "    reid_system = process_video_with_yolo_detections(\n",
    "        video_path, detections_file, output_video_path\n",
    "    )\n",
    "    \n",
    "    print(\"Processing complete!\")\n",
    "    print(f\"Results saved to: tracking_results.json\")\n",
    "    print(f\"Output video saved to: {output_video_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
